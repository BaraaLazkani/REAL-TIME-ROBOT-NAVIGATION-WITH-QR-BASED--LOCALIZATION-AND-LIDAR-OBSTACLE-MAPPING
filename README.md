# Real-Time Robot Navigation with QR-Based Localization and LIDAR Obstacle Mapping

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Arduino](https://img.shields.io/badge/Arduino-Uno%20R3-00979D.svg)](https://www.arduino.cc/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)](https://opencv.org/)
[![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi-4-C51A4A.svg)](https://www.raspberrypi.org/)

> **An autonomous mobile robot navigation system integrating visual localization, sensor fusion, and LIDAR-based obstacle avoidance for indoor environments.**

<p align="center">
  <a href="https://www.youtube.com/playlist?list=PLPltBpN1wF6AVFg1B2o3nOTB_6zbEckF2">
    <b>ğŸ“º Watch Demo Videos on YouTube</b>
  </a>
</p>

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Documentation](#documentation)
- [Hardware Requirements](#hardware-requirements)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Tasks](#tasks)
- [Configuration](#configuration)
- [Results](#results)
- [Simulation](#simulation)
- [Team](#team)
- [License](#license)
- [Citation](#citation)

---

## Overview

This project demonstrates a **hybrid computational architecture** for autonomous robot navigation:

| Component | Role | Key Responsibilities |
|-----------|------|---------------------|
| **Raspberry Pi 4** | High-level perception | ArUco detection, pose estimation, LIDAR processing |
| **Arduino Uno R3** | Low-level control | Kalman filtering, sensor fusion, motor control |

### System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raspberry Pi â”‚  USB    â”‚   Arduino    â”‚
â”‚ (Vision +    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (Control +  â”‚
â”‚  LIDAR)      â”‚ Serial  â”‚   Fusion)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
   â–¼        â–¼        â–¼    â–¼         â–¼     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Cameraâ”‚â”‚LIDAR â”‚â”‚ USB  â”‚â”‚ IMU  â”‚â”‚Motor â”‚â”‚Motorsâ”‚
â”‚16MP  â”‚â”‚A1M8  â”‚â”‚Serialâ”‚â”‚6050  â”‚â”‚Driverâ”‚â”‚ 2x   â”‚
â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜
```

**Why this architecture?**
- **Raspberry Pi**: Leverages computational power for computer vision tasks
- **Arduino**: Ensures real-time, deterministic control loops (67 Hz)
- **Serial Communication**: Low-latency data exchange (9600-115200 baud)

---

## Key Features

### âœ¨ Core Capabilities

- **ğŸ¯ Visual Localization**: ArUco marker detection provides absolute position references
- **ğŸ”„ Sensor Fusion**: Kalman filter fuses visual and inertial (IMU) measurements
- **ğŸš§ Obstacle Avoidance**: LIDAR-based dynamic path planning (inspired by DWA)
- **ğŸ“ Waypoint Navigation**: Smooth trajectory following with proportional control
- **âš¡ Real-time Performance**: Distributed processing for responsive control

### ğŸ”¬ Technical Highlights

- **Localization Accuracy**: < 10 cm position error, < 5Â° heading error
- **Update Rates**: 10 Hz visual updates, 67 Hz control loop
- **Obstacle Detection**: 360Â° coverage, 1.5m range, Â±20Â° safety inflation
- **Success Rate**: 95% waypoint accuracy, 90% goal reaching (Task 2)

---

## Documentation

### ğŸ“š Complete Documentation Set

| Document | Description | Audience |
|----------|-------------|----------|
| **[README.md](README.md)** (this file) | Overview & quick start | All users |
| **[TECHNICAL.md](TECHNICAL.md)** | Algorithms, math, results | Researchers, developers |
| **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** | System diagrams, data flow | Developers, integrators |
| **[CLAUDE.md](CLAUDE.md)** | AI development guide | AI assistants, contributors |
| **[Report (PDF)](docs/report.pdf)** | Full academic report | Academic readers |
| **[Calibration Guide](calibration/README.md)** | Camera calibration | Setup, maintenance |

### ğŸ¥ Video Demonstrations

**[Watch on YouTube Playlist](https://www.youtube.com/playlist?list=PLPltBpN1wF6AVFg1B2o3nOTB_6zbEckF2)**

The demonstration videos showcase:
- Real-time ArUco marker tracking
- Kalman filter pose estimation
- Waypoint navigation execution
- LIDAR obstacle avoidance

See [docs/media/README.md](docs/media/README.md) for video descriptions.

---

## Hardware Requirements

### Essential Components

| Component | Specification | Purpose |
|-----------|--------------|---------|
| **Raspberry Pi 4** | 2GB+ RAM | Vision processing, LIDAR |
| **Arduino Uno R3** | ATmega328P | Real-time control |
| **Arducam** | 16MP CSI | Visual localization |
| **MPU6050** | 6-axis IMU | Orientation sensing |
| **RPLIDAR A1M8** | 360Â° 2D | Obstacle detection |
| **L298 Motor Driver** | Dual H-bridge | Motor control |
| **DC Motors** | 1:48 gear ratio, 2x | Differential drive |
| **Power Supply** | 12V, 2A+ | Motors & electronics |

### Optional Accessories

- Differential-drive chassis
- Printed ArUco markers (96mm, 4Ã—4 dictionary)
- Camera mount & robot frame
- Power distribution board

**Estimated Cost**: ~$200-250 USD (excluding chassis)

---

## Quick Start

### 1ï¸âƒ£ Software Setup

**On Raspberry Pi:**

```bash
# Clone repository
git clone https://github.com/BaraaLazkani/REAL-TIME-ROBOT-NAVIGATION-WITH-QR-BASED--LOCALIZATION-AND-LIDAR-OBSTACLE-MAPPING.git
cd REAL-TIME-ROBOT-NAVIGATION-WITH-QR-BASED--LOCALIZATION-AND-LIDAR-OBSTACLE-MAPPING

# Install dependencies
pip3 install -r requirements.txt

# Run automated setup (optional)
bash scripts/setup_environment.sh

# Calibrate camera
cd calibration
python3 capture_images.py    # Capture calibration images
python3 calibrate_camera.py  # Compute calibration
```

**On Arduino:**

1. Install Arduino IDE
2. Install libraries: `BasicLinearAlgebra`, `MPU6050_light`
3. Open `src/arduino/task1_aruco_nav/task1_aruco_nav.ino`
4. Update marker positions in code (lines 30-35)
5. Upload to Arduino Uno R3

### 2ï¸âƒ£ Configuration

Edit YAML files in `config/` directory:

```bash
config/
â”œâ”€â”€ camera_calibration.yaml  # Camera matrix & distortion
â”œâ”€â”€ robot_params.yaml        # Robot dimensions, motor pins
â”œâ”€â”€ task1_config.yaml        # Markers, waypoints, gains
â”œâ”€â”€ task2_config.yaml        # Markers, goal, LIDAR params
â””â”€â”€ serial_config.yaml       # Ports, baud rates
```

**Critical**: Update marker positions to match your physical setup!

### 3ï¸âƒ£ Run the System

**Task 1: Waypoint Navigation**

```bash
cd src/raspberry_pi
python3 task1_aruco_localization.py
```

**Task 2: LIDAR Navigation with Obstacles**

```bash
cd src/raspberry_pi
python3 task2_slam_navigation.py
```

Press **ESC** to stop gracefully.

---

## Project Structure

```
robot-navigation/
â”œâ”€â”€ ğŸ“– README.md                  # This file
â”œâ”€â”€ ğŸ“˜ TECHNICAL.md               # Detailed algorithms & results
â”œâ”€â”€ ğŸ“— CLAUDE.md                  # AI development guide
â”œâ”€â”€ ğŸ“œ LICENSE                    # CC BY-NC-SA 4.0
â”œâ”€â”€ ğŸ“‹ requirements.txt           # Python dependencies
â”‚
â”œâ”€â”€ âš™ï¸  config/                    # YAML configuration files
â”‚   â”œâ”€â”€ camera_calibration.yaml
â”‚   â”œâ”€â”€ robot_params.yaml
â”‚   â”œâ”€â”€ task1_config.yaml
â”‚   â”œâ”€â”€ task2_config.yaml
â”‚   â””â”€â”€ serial_config.yaml
â”‚
â”œâ”€â”€ ğŸ“š docs/                      # Documentation & media
â”‚   â”œâ”€â”€ ğŸ“„ report.pdf             # Full technical report
â”‚   â”œâ”€â”€ ğŸ“Š presentation.pptx      # Project presentation
â”‚   â”œâ”€â”€ ğŸ—ï¸  ARCHITECTURE.md        # System diagrams
â”‚   â””â”€â”€ ğŸ¬ media/                 # Demo videos
â”‚
â”œâ”€â”€ ğŸ src/                       # Source code
â”‚   â”œâ”€â”€ raspberry_pi/            # Python code
â”‚   â”‚   â”œâ”€â”€ common/              # Reusable modules
â”‚   â”‚   â”‚   â”œâ”€â”€ camera.py
â”‚   â”‚   â”‚   â”œâ”€â”€ aruco_detector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ serial_comm.py
â”‚   â”‚   â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ task1_aruco_localization.py
â”‚   â”‚   â”œâ”€â”€ task2_slam_navigation.py
â”‚   â”‚   â”œâ”€â”€ lidar_processor.py
â”‚   â”‚   â””â”€â”€ visualize_lidar.py
â”‚   â”‚
â”‚   â””â”€â”€ arduino/                 # Arduino code
â”‚       â”œâ”€â”€ common/              # Shared headers
â”‚       â”‚   â”œâ”€â”€ kalman_filter.h
â”‚       â”‚   â”œâ”€â”€ motor_control.h
â”‚       â”‚   â””â”€â”€ navigation.h
â”‚       â”œâ”€â”€ task1_aruco_nav/
â”‚       â””â”€â”€ task2_slam_nav/
â”‚
â”œâ”€â”€ ğŸ”¬ calibration/               # Camera calibration tools
â”‚   â”œâ”€â”€ calibrate_camera.py
â”‚   â”œâ”€â”€ capture_images.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ® Simulation/                # Python simulation environment
â”‚   â”œâ”€â”€ RobotSim.py              # Simulation engine
â”‚   â”œâ”€â”€ RobotControl.py          # Control integration
â”‚   â”œâ”€â”€ KalmanFilter.py          # Kalman filter
â”‚   â”œâ”€â”€ DiffDriveController.py   # Diff drive controller
â”‚   â”œâ”€â”€ shortestpath.py          # Dijkstra's algorithm
â”‚   â”œâ”€â”€ generate_plots.py        # Visualization generator
â”‚   â”œâ”€â”€ params.yaml              # Simulation config
â”‚   â””â”€â”€ README.md                # Simulation guide
â”‚
â””â”€â”€ ğŸ› ï¸  scripts/                   # Utility scripts
    â””â”€â”€ setup_environment.sh     # Raspberry Pi auto-setup
```

---

## Tasks

### Task 1: ArUco Marker-Based Localization

Navigate through predefined waypoints using visual markers.

**Algorithm:**
1. Detect ArUco markers in camera feed
2. Estimate 6-DOF pose using `solvePnPRansac`
3. Transform to robot frame
4. Fuse with IMU via Kalman filter
5. Navigate to waypoints with proportional controller

**Performance:**
- âœ… Position accuracy: < 10 cm
- âœ… Heading accuracy: < 5Â°
- âœ… Waypoint success: 95%

### Task 2: SLAM with LIDAR Navigation

Navigate to goal while avoiding obstacles.

**Algorithm:**
1. Segment LIDAR scan into 5Â° sectors
2. Detect obstacles within 1.5m
3. Apply Â±20Â° safety inflation
4. Select local goal in free direction closest to global goal
5. Navigate iteratively, recomputing every 0.5m

**Performance:**
- âœ… Goal reaching: 90% success
- âœ… Collision-free: 90% (18/20 runs)
- âœ… Path efficiency: 87%

---

## Configuration

All system parameters are **externalized to YAML files**â€”no code changes needed!

### Example: Modify Navigation Gains

```yaml
# config/task1_config.yaml
navigation:
  kp: 0.9     # Distance gain (increase for faster approach)
  ka: 20.0    # Angle gain (increase for sharper turns)
  threshold: 0.25  # Goal reached threshold (meters)
```

### Example: Add New Marker

```yaml
# config/task1_config.yaml
markers:
  - id: 12
    x: 2.5
    y: 1.8
    theta: -1.5708  # -Ï€/2 radians
```

See [TECHNICAL.md](TECHNICAL.md) for detailed parameter explanations.

---

## Results

### Localization Performance

| Metric | Raw ArUco | Kalman Filtered | Improvement |
|--------|-----------|-----------------|-------------|
| **Position Error (RMS)** | 12.3 cm | 8.7 cm | **29%** |
| **Heading Error (RMS)** | 7.2Â° | 4.1Â° | **43%** |

### Task Comparison

| Metric | Task 1 | Task 2 |
|--------|--------|--------|
| **Success Rate** | 95% | 90% |
| **Average Error** | < 10 cm | < 12 cm |
| **Path Efficiency** | 100% (direct) | 87% (with obstacles) |
| **Control Loop Rate** | 67 Hz | 67 Hz |
| **Vision Update Rate** | 15-20 Hz | 10-15 Hz |

### Sample Trajectory

*[See demonstration videos in docs/media/ for visual results]*

For detailed experimental results, plots, and analysis, see **[TECHNICAL.md](TECHNICAL.md#experimental-results)**.

---

## Simulation

### ğŸ® Python-Based Virtual Environment

Before deploying to hardware, algorithms were tested and validated in a **Python simulation environment**. The simulation implements the complete navigation stack (Dijkstra path planning, Kalman filtering, differential drive control) in a virtual testbed.

**ğŸ“º Watch the simulation in action:** [YouTube Demo](https://youtu.be/W2GyEFLIgFA)

The `Simulation/` directory contains the source code and **7 comprehensive visualizations** including:

| Visualization | Description |
|---------------|-------------|
| **Environment Map** | Obstacle layout + marker positions |
| **Path Planning** | Dijkstra's algorithm visualization |
| **Trajectory Analysis** | Ground truth vs Kalman comparison |
| **Controller Analysis** | Velocity profiles and control signals |
| **Sensor Fusion** | IMU + ArUco â†’ Kalman filter |
| **Error Metrics** | Comprehensive error analysis |
| **Path Scenarios** | 4 different navigation scenarios |

### Key Features

- **ğŸ—ºï¸ Path Planning**: Dijkstra's algorithm for shortest obstacle-free path
- **ğŸ“Š State Estimation**: Kalman filter with simulated sensor noise
- **ğŸ¯ Diff Drive Control**: Proportional controller for navigation
- **ğŸ“ˆ Visualization**: Real-time trajectory plotting and error analysis

### Simulation Results

#### Environment & Path Planning

<p align="center">
  <img src="Simulation/plots/simulation_environment.png" alt="Simulation Environment" width="45%"/>
  <img src="Simulation/plots/simulation_path_planning.png" alt="Path Planning" width="45%"/>
</p>

#### Performance Analysis

<p align="center">
  <img src="Simulation/plots/simulation_sensor_fusion.png" alt="Sensor Fusion" width="48%"/>
  <img src="Simulation/plots/simulation_controller.png" alt="Controller Analysis" width="48%"/>
</p>

<p align="center">
  <img src="Simulation/plots/simulation_error_analysis.png" alt="Error Analysis" width="48%"/>
  <img src="Simulation/plots/simulation_path_scenarios.png" alt="Path Scenarios" width="48%"/>
</p>

<details>
<summary><b>ğŸ“Š Click to see Trajectory Comparison</b></summary>
<br>
<p align="center">
  <img src="Simulation/plots/simulation_trajectory.png" alt="Trajectory Analysis" width="90%"/>
</p>
</details>

### Why Simulate?

âœ… **Safe Testing**: Validate algorithms before hardware deployment
âœ… **Parameter Tuning**: Optimize controller gains risk-free
âœ… **Edge Cases**: Test scenarios difficult to reproduce physically
âœ… **Ground Truth**: Compare estimates against perfect position knowledge

See **[Simulation/README.md](Simulation/README.md)** for complete documentation and usage instructions.

---

## Team

ğŸ‘¥ **Contributors:**

- **Alaa Hussein**
- **Baraa Lazkani**
- **Haidar Saad**
- **Humam Yehia**

ğŸ“ **Supervised by:**

- Dr. Fadi Muttawag
- Eng. Baher Kher-Bek

---

## License

This work is licensed under **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International** with additional citation requirements.

### ğŸš« Restrictions

- âŒ **No Commercial Use** without explicit written permission
- âœ… **Academic & Educational Use** freely permitted
- ğŸ“ **Citation Required** for all uses

See [LICENSE](LICENSE) for full terms.

---

## Citation

If you use this work in research, publications, or derivative projects, please cite:

```bibtex
@misc{hussein2024robot,
  title={Real-Time Robot Navigation with QR-Based Localization and LIDAR Obstacle Mapping},
  author={Hussein, Alaa and Lazkani, Baraa and Saad, Haidar and Yehia, Humam},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/BaraaLazkani/REAL-TIME-ROBOT-NAVIGATION-WITH-QR-BASED--LOCALIZATION-AND-LIDAR-OBSTACLE-MAPPING}},
  note={Supervised by Dr. Fadi Muttawag and Eng. Baher Kher-Bek}
}
```

**APA Format:**
> Hussein, A., Lazkani, B., Saad, H., & Yehia, H. (2024). *Real-Time Robot Navigation with QR-Based Localization and LIDAR Obstacle Mapping*. GitHub repository. https://github.com/BaraaLazkani/REAL-TIME-ROBOT-NAVIGATION-WITH-QR-BASED--LOCALIZATION-AND-LIDAR-OBSTACLE-MAPPING

---

## Acknowledgments

This project utilizes:
- **OpenCV** ArUco module for marker detection
- **BasicLinearAlgebra** library for Arduino matrix operations
- **RPLidar SDK** for LIDAR interfacing
- **Picamera2** for Raspberry Pi camera control

Special thanks to the open-source robotics community.

---

## Getting Help

- ğŸ“– Read the [Technical Documentation](TECHNICAL.md) for implementation details
- ğŸ—ï¸ Check [Architecture Diagrams](docs/ARCHITECTURE.md) for system understanding
- ğŸ“š Review the [Full Report](docs/report.pdf) for theoretical background
- ğŸ¬ Watch [Demo Videos](https://www.youtube.com/playlist?list=PLPltBpN1wF6AVFg1B2o3nOTB_6zbEckF2) for visual examples

---

<p align="center">
  <b>Built with â¤ï¸ for autonomous robotics research</b>
</p>

<p align="center">
  <a href="TECHNICAL.md">ğŸ“˜ Technical Docs</a> â€¢
  <a href="docs/ARCHITECTURE.md">ğŸ—ï¸ Architecture</a> â€¢
  <a href="docs/report.pdf">ğŸ“„ Report</a> â€¢
  <a href="https://www.youtube.com/playlist?list=PLPltBpN1wF6AVFg1B2o3nOTB_6zbEckF2">ğŸ¬ Videos</a>
</p>
